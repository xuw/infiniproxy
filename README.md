# OpenAI to Claude API Proxy

A local proxy server that translates between OpenAI-compatible API format and Claude (Anthropic) API format, allowing Claude Code and other Claude-based tools to use OpenAI-compatible backends.

## Features

- ‚úÖ **Dual API support**: Both Claude API format and OpenAI API format
- ‚úÖ **Translation mode**: Claude API requests ‚Üí OpenAI backend (with translation)
- ‚úÖ **Pass-through mode**: OpenAI API requests ‚Üí OpenAI backend (no translation)
- ‚úÖ **Multi-user support**: API key authentication for multiple users
- ‚úÖ **Usage tracking**: Track token usage per API key
- ‚úÖ **Admin endpoints**: User and API key management
- ‚úÖ **Web-based Admin UI**: Beautiful interface for managing users and viewing usage
- ‚úÖ Handles system messages properly
- ‚úÖ Supports multi-turn conversations
- ‚úÖ Handles Claude's content blocks
- ‚úÖ **Full tool/function calling support**
- ‚úÖ Special support for reasoning models (like glm-4.6)
- ‚úÖ Comprehensive test suite
- ‚úÖ Easy configuration via environment variables

## Architecture

The proxy supports two modes:

### Translation Mode (Claude API ‚Üí OpenAI Backend)
```
Claude Code ‚Üí HTTP (Claude Format) ‚Üí Proxy Server ‚Üí HTTP (OpenAI Format) ‚Üí OpenAI-Compatible Backend
                                      /v1/messages
                                           ‚Üì (translate)
Claude Code ‚Üê HTTP (Claude Format) ‚Üê Proxy Server ‚Üê HTTP (OpenAI Format) ‚Üê OpenAI-Compatible Backend
```

### Pass-Through Mode (OpenAI API ‚Üí OpenAI Backend)
```
OpenAI Client ‚Üí HTTP (OpenAI Format) ‚Üí Proxy Server ‚Üí HTTP (OpenAI Format) ‚Üí OpenAI-Compatible Backend
                                        /v1/chat/completions
                                             ‚Üì (pass-through)
OpenAI Client ‚Üê HTTP (OpenAI Format) ‚Üê Proxy Server ‚Üê HTTP (OpenAI Format) ‚Üê OpenAI-Compatible Backend
```

## Installation

1. Clone or download this repository
2. Create a virtual environment and install dependencies:

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

3. Configure the environment variables:

```bash
# Copy the example .env file
cp .env.example .env

# Edit .env with your settings
nano .env
```

## Configuration

Edit the `.env` file with your settings:

```bash
# OpenAI-compatible backend configuration
OPENAI_BASE_URL=https://cloud.infini-ai.com/maas/v1/chat/completions
OPENAI_API_KEY=your-api-key-here
OPENAI_MODEL=glm-4.6

# Proxy server configuration
PROXY_HOST=localhost
PROXY_PORT=8000

# Optional settings
TIMEOUT=300
DEBUG=false
```

## Usage

### Starting the Proxy Server

```bash
# Activate virtual environment
source venv/bin/activate

# Run the proxy server
python proxy_server.py
```

The server will start on `http://localhost:8000` (or your configured port).

### Web-Based Admin Interface

The easiest way to manage users and API keys is through the web-based admin interface:

**Access the Admin UI:**
```
http://localhost:8000/admin
```

The admin interface provides:
- üë• **Users Tab**: Create and view all users
- üîë **API Keys Tab**: Generate, view, and deactivate API keys
- üìä **Usage Statistics Tab**: View token usage per API key

**Features:**
- Beautiful, responsive interface
- Real-time data updates
- Secure API key generation (shown only once)
- Usage tracking visualization
- Filter API keys by user
- One-click key deactivation

### Command-Line User and API Key Management

You can also manage users and keys via command-line:

Before you can use the proxy, you need to create a user and API key:

**1. Create a user:**
```bash
curl -X POST "http://localhost:8000/admin/users?username=alice&email=alice@example.com"
```

Response:
```json
{
  "success": true,
  "user_id": 1,
  "username": "alice",
  "message": "User alice created successfully"
}
```

**2. Create an API key for the user:**
```bash
curl -X POST "http://localhost:8000/admin/api-keys?user_id=1&name=my-key"
```

Response:
```json
{
  "success": true,
  "api_key": "sk-abc123...",
  "user_id": 1,
  "name": "my-key",
  "warning": "Save this API key! It will not be shown again."
}
```

**Important:** Save the API key immediately - it cannot be retrieved later!

**3. List all users:**
```bash
curl http://localhost:8000/admin/users
```

**4. List API keys:**
```bash
curl http://localhost:8000/admin/api-keys
# Or for a specific user:
curl "http://localhost:8000/admin/api-keys?user_id=1"
```

**5. Deactivate an API key:**
```bash
curl -X DELETE http://localhost:8000/admin/api-keys/1
```

### Using with Claude Code

Configure Claude Code to use the proxy as its API endpoint:

1. Set the API endpoint to: `http://localhost:8000`
2. Set the API key to your generated key (from the API key creation step above)
3. The proxy accepts Claude API format at `/v1/messages`

### Using with OpenAI-Compatible Clients

You can use the proxy with any OpenAI-compatible client or library:

**Python (OpenAI SDK):**
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="sk-abc123..."  # Your API key from the key creation step
)

response = client.chat.completions.create(
    model="glm-4.6",
    messages=[
        {"role": "user", "content": "Hello!"}
    ]
)
```

**JavaScript/TypeScript (OpenAI SDK):**
```javascript
import OpenAI from 'openai';

const client = new OpenAI({
    baseURL: 'http://localhost:8000/v1',
    apiKey: 'sk-abc123...'  // Your API key
});

const response = await client.chat.completions.create({
    model: 'glm-4.6',
    messages: [{ role: 'user', content: 'Hello!' }]
});
```

**cURL:**
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-abc123..." \
  -d '{
    "model": "glm-4.6",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

### API Endpoints

#### `GET /`
Returns server information and available endpoints.

```bash
curl http://localhost:8000/
```

#### `GET /health`
Health check endpoint.

```bash
curl http://localhost:8000/health
```

#### `POST /v1/messages`
Claude API endpoint (with translation to OpenAI format).

This endpoint accepts Claude API format requests, translates them to OpenAI format,
and returns responses in Claude format.

**Requires authentication via Bearer token.**

```bash
curl -X POST http://localhost:8000/v1/messages \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-abc123..." \
  -d '{
    "model": "claude-3-5-sonnet-20241022",
    "max_tokens": 1024,
    "messages": [
      {
        "role": "user",
        "content": "Hello!"
      }
    ]
  }'
```

#### `POST /v1/chat/completions`
OpenAI API endpoint (pass-through mode).

This endpoint accepts OpenAI API format requests and passes them directly to the
backend without translation, returning OpenAI format responses.

**Requires authentication via Bearer token.**

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-abc123..." \
  -d '{
    "model": "glm-4.6",
    "max_tokens": 1024,
    "messages": [
      {
        "role": "user",
        "content": "Hello!"
      }
    ]
  }'
```

### Usage Tracking Endpoints

#### `GET /usage/me`
Get usage statistics for the authenticated user.

```bash
curl http://localhost:8000/usage/me \
  -H "Authorization: Bearer sk-abc123..."

# With date range:
curl "http://localhost:8000/usage/me?start_date=2024-01-01T00:00:00&end_date=2024-12-31T23:59:59" \
  -H "Authorization: Bearer sk-abc123..."
```

Response:
```json
{
  "user_id": 1,
  "username": "alice",
  "total_requests": 42,
  "total_input_tokens": 1500,
  "total_output_tokens": 3000,
  "total_tokens": 4500,
  "usage_by_endpoint": [
    {
      "endpoint": "/v1/chat/completions",
      "model": "glm-4.6",
      "total_requests": 25,
      "total_input_tokens": 900,
      "total_output_tokens": 1800,
      "total_tokens": 2700
    }
  ]
}
```

### Admin Endpoints

#### `POST /admin/users`
Create a new user (shown above in User Management section).

#### `POST /admin/api-keys`
Create an API key for a user (shown above in User Management section).

#### `GET /admin/users`
List all users.

#### `GET /admin/api-keys`
List all API keys or keys for a specific user.

#### `DELETE /admin/api-keys/{api_key_id}`
Deactivate an API key.

#### `GET /usage/api-key/{api_key_id}`
Get usage statistics for a specific API key.

## Testing

### Unit Tests

Run the comprehensive test suite:

```bash
source venv/bin/activate
pytest tests/ -v
```

### End-to-End Tests

Test the proxy with real API calls:

```bash
# Make sure the proxy server is running first
python proxy_server.py

# In another terminal:
source venv/bin/activate
python test_e2e.py
```

### Manual Testing

You can also use the test script to verify the OpenAI endpoint works:

```bash
source venv/bin/activate
python test_api.py
```

## API Translation Details

### Request Translation (Claude ‚Üí OpenAI)

The proxy translates Claude API requests to OpenAI format:

- **System messages**: Extracted from `system` field and added as first message with `role: "system"`
- **Content blocks**: Claude's content blocks (e.g., `[{"type": "text", "text": "..."}]`) are converted to simple strings
- **Parameters**: `max_tokens`, `temperature`, `top_p`, `stop_sequences` are mapped appropriately

### Response Translation (OpenAI ‚Üí Claude)

OpenAI responses are translated back to Claude format:

- **Message structure**: Wrapped in Claude's content block format
- **Stop reasons**: Mapped from OpenAI's `finish_reason` to Claude's `stop_reason`
  - `stop` ‚Üí `end_turn`
  - `length` ‚Üí `max_tokens`
  - `content_filter` ‚Üí `content_filtered`
- **Usage tokens**: Mapped from `prompt_tokens`/`completion_tokens` to `input_tokens`/`output_tokens`
- **Reasoning content**: Special handling for models with `reasoning_content` (like glm-4.6)

## Project Structure

```
infiniproxy/
‚îú‚îÄ‚îÄ config.py              # Configuration management
‚îú‚îÄ‚îÄ translator.py          # Request/response translation logic
‚îú‚îÄ‚îÄ openai_client.py       # OpenAI API client
‚îú‚îÄ‚îÄ user_manager.py        # User and API key management
‚îú‚îÄ‚îÄ proxy_server.py        # Main proxy server (FastAPI)
‚îú‚îÄ‚îÄ proxy_users.db         # SQLite database (not in git)
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ .env                   # Configuration (not in git)
‚îú‚îÄ‚îÄ .gitignore            # Git ignore patterns
‚îú‚îÄ‚îÄ DESIGN.md             # Architecture and design document
‚îú‚îÄ‚îÄ README.md             # This file
‚îú‚îÄ‚îÄ test_api.py           # API verification script
‚îú‚îÄ‚îÄ test_e2e.py           # End-to-end tests
‚îú‚îÄ‚îÄ static/               # Admin UI assets
‚îÇ   ‚îú‚îÄ‚îÄ admin.html        # Admin interface HTML
‚îÇ   ‚îî‚îÄ‚îÄ admin.js          # Admin interface JavaScript
‚îî‚îÄ‚îÄ tests/                # Unit tests
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ test_translator.py
    ‚îî‚îÄ‚îÄ test_proxy_server.py
```

## Limitations

- **Streaming**: Currently not fully implemented. Streaming requests fall back to non-streaming responses.
- **Images**: Image content blocks are not yet supported in translation.

## Troubleshooting

### Connection Refused
- Make sure the proxy server is running
- Check that the port (default 8000) is not in use by another application
- Verify firewall settings allow connections to localhost

### API Key Errors
- Verify your `OPENAI_API_KEY` in the `.env` file is correct
- Check that the OpenAI-compatible endpoint is accessible

### Translation Errors
- Enable debug logging by setting `DEBUG=true` in `.env`
- Check the server logs for detailed error messages
- Verify the request format matches Claude API specifications

### Tests Failing
- Ensure all dependencies are installed: `pip install -r requirements.txt`
- For end-to-end tests, make sure the proxy server is running
- Check that the OpenAI backend endpoint is accessible

## Development

### Running in Development Mode

```bash
# Enable debug logging
export DEBUG=true

# Run with auto-reload
uvicorn proxy_server:app --reload --host localhost --port 8000
```

### Adding New Features

1. Update `translator.py` for translation logic changes
2. Update `openai_client.py` for client changes
3. Add tests in `tests/` directory
4. Update documentation

## License

This project is provided as-is for educational and development purposes.

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## Support

For issues or questions:

1. Check the troubleshooting section
2. Review the DESIGN.md for architecture details
3. Open an issue on the repository
